{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ddayv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ddayv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random as rd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from nltk import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import autokeras as ak\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import nltk\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(f):     ## Pre-processando a frase\n",
    "\n",
    "    ## Colocando em minusculo\n",
    "    ## Retirando a pontuaçao\n",
    "    ## Retirando as StopWords\n",
    "    \n",
    "    f = f.lower().replace('\\n', '').replace('-','').replace('#','').replace('.','').replace(',','').replace('!','').replace('r\\n','').replace('  ','').replace('https','').replace('rt','').replace('rn','')\n",
    "    token = RegexpTokenizer(r\"\\w+\")\n",
    "    f = token.tokenize(f)\n",
    "    \n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    \n",
    "    new_word = [word for word in f if not word in stop_words]\n",
    "    \n",
    "    return ' '.join(new_word)\n",
    "\n",
    "def remove_user(frase):\n",
    "    frase = re.sub('@\\w+','',frase)\n",
    "    frase = re.sub('{https}[^ ]+','',frase)\n",
    "    frase = re.sub('https\\w+','',frase)\n",
    "    # re.sub('#\\w+','',frase)\n",
    "    return frase\n",
    "\n",
    "def pre_X(frases):\n",
    "    lista = []\n",
    "    \n",
    "    for frase in frases:\n",
    "        lista.append(frase)\n",
    "        \n",
    "    return lista\n",
    "\n",
    "def pre_Y(number):\n",
    "    lista = []\n",
    "    \n",
    "    for numb in number:\n",
    "        lista.append(numb)\n",
    "    \n",
    "    return lista\n",
    "\n",
    "def set_array(frases):\n",
    "    \n",
    "    vocab = []\n",
    "    palavras = []\n",
    "    for frase in frases:\n",
    "        \n",
    "        text_array = remove_user(frase)\n",
    "        text_array = Tokenize(text_array)\n",
    "        text_array = text_array.split(' ')\n",
    "        for i in range(len(text_array)):\n",
    "            vocab.append(text_array[i])\n",
    "    \n",
    "    \n",
    "        \n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TWEETSENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_twitter</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1343</td>\n",
       "      <td>863044774588272640</td>\n",
       "      <td>Que coisa linda! O Programa #encontro estava m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1344</td>\n",
       "      <td>865583716088766467</td>\n",
       "      <td>Por mais #Encontro com as Irmãs Galvão, adorei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1345</td>\n",
       "      <td>865063232201011201</td>\n",
       "      <td>Mr. CATRA @OficialMrCatra lançando sua nova mú...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1346</td>\n",
       "      <td>864668391008763905</td>\n",
       "      <td>quem viu aquela lutadora modela barbuda tatuad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1347</td>\n",
       "      <td>865572794016378882</td>\n",
       "      <td>Tô passada com esse cara.... quanta merda pode...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11565</th>\n",
       "      <td>12908</td>\n",
       "      <td>864636619000877056</td>\n",
       "      <td>eu ja to aqui pronto pro #MasterChefBR mas ain...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11566</th>\n",
       "      <td>12909</td>\n",
       "      <td>863581588713603072</td>\n",
       "      <td>MALUCO! Uma coisa que eu não tenho coragem é e...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11567</th>\n",
       "      <td>12910</td>\n",
       "      <td>864831041349054464</td>\n",
       "      <td>#MaisVoce @ANAMARIABRAGA  está linda @RedeGlobo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11568</th>\n",
       "      <td>12911</td>\n",
       "      <td>863042798575951872</td>\n",
       "      <td>Que orgulho de ti, @sportrecife! #Encontro</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11569</th>\n",
       "      <td>12912</td>\n",
       "      <td>864681041541386240</td>\n",
       "      <td>Gente olha o bíceps desse padre #ConversaComBial</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11570 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id          id_twitter  \\\n",
       "0       1343  863044774588272640   \n",
       "1       1344  865583716088766467   \n",
       "2       1345  865063232201011201   \n",
       "3       1346  864668391008763905   \n",
       "4       1347  865572794016378882   \n",
       "...      ...                 ...   \n",
       "11565  12908  864636619000877056   \n",
       "11566  12909  863581588713603072   \n",
       "11567  12910  864831041349054464   \n",
       "11568  12911  863042798575951872   \n",
       "11569  12912  864681041541386240   \n",
       "\n",
       "                                                    text sentiment  \n",
       "0      Que coisa linda! O Programa #encontro estava m...         1  \n",
       "1      Por mais #Encontro com as Irmãs Galvão, adorei...         1  \n",
       "2      Mr. CATRA @OficialMrCatra lançando sua nova mú...         1  \n",
       "3      quem viu aquela lutadora modela barbuda tatuad...         0  \n",
       "4      Tô passada com esse cara.... quanta merda pode...        -1  \n",
       "...                                                  ...       ...  \n",
       "11565  eu ja to aqui pronto pro #MasterChefBR mas ain...        -1  \n",
       "11566  MALUCO! Uma coisa que eu não tenho coragem é e...        -1  \n",
       "11567    #MaisVoce @ANAMARIABRAGA  está linda @RedeGlobo         1  \n",
       "11568         Que orgulho de ti, @sportrecife! #Encontro         1  \n",
       "11569   Gente olha o bíceps desse padre #ConversaComBial         1  \n",
       "\n",
       "[11570 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweet= pd.read_csv('export_TweetSentBR.csv')\n",
    "df_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove = df_tweet[df_tweet['sentiment'] == '-']\n",
    "df_tweet = df_tweet.drop(df_remove.index)\n",
    "\n",
    "df_tweet['text'] = df_tweet['text'].apply(remove_user)\n",
    "df_tweet['text'] = df_tweet['text'].apply(Tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['sentiment'] = df_tweet['sentiment'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_Tweetsent = df_tweet['text']\n",
    "polarity = np.asarray(df_tweet['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11533, 25852)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text = count_vect.transform(text_Tweetsent)\n",
    "\n",
    "X_text_transform = tfidf_transformer.transform(X_text) # Aplicando o TF-IDF\n",
    "X_text_transform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test_TWEET, Y_train1, Y_test_TWEET = train_test_split(X_text_transform, polarity, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train12, X_test_TWEET1, Y_train12, Y_test_TWEET1 = train_test_split(df_tweet['text'], df_tweet['sentiment'] , test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"tash-pt.csv\")\n",
    "df1 = df.dropna()\n",
    "df1 = df.drop(columns=['id_twitter'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['text'] = df1['text'].apply(remove_user)\n",
    "df1['text'] = df1['text'].apply(Tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2787, 25852)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_TASH = df1['text']\n",
    "sentiment = np.asarray(df1['sentiment'])\n",
    "\n",
    "X_text_TASH = count_vect.transform(text_TASH)\n",
    "X_text_TASH_ = tfidf_transformer.transform(X_text_TASH) # Aplicando o TF-IDF\n",
    "X_text_TASH_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test_TASH, Y_train2, Y_test_TASH = train_test_split(X_text_TASH_, sentiment, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train21, X_test_TASH21, Y_train21, Y_test_TASH21 = train_test_split(df['text'], np.asarray(df['sentiment']), test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KANSOAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.txt\", sep=',', header=None, names=['text','sentiment'])\n",
    "\n",
    "df_remove = df[df['sentiment'] == '#Inveja'].index\n",
    "df = df.drop(df_remove)\n",
    "\n",
    "df_remove = df[df['sentiment'] == '#Raiva'].index\n",
    "df = df.drop(df_remove)\n",
    "\n",
    "df_remove = df[df['sentiment'] == '#Ironia'].index\n",
    "df = df.drop(df_remove)\n",
    "\n",
    "\n",
    "def binario(termo):\n",
    "    if termo == '#Feliz' or termo == '#Amor':\n",
    "        return 1\n",
    "    elif termo == '#Triste' or termo == '#Chateado':\n",
    "        return 0       \n",
    "    \n",
    "df['sentiment'] = df['sentiment'].apply(binario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(remove_user)\n",
    "df['text'] = df['text'].apply(Tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9224, 25852)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_KANSOAN = df['text']\n",
    "sentiment = np.asarray(df['sentiment'])\n",
    "\n",
    "X_text_KANSOAN = count_vect.transform(text_KANSOAN)\n",
    "X_text_KANSOAN_ = tfidf_transformer.transform(X_text_KANSOAN) # Aplicando o TF-IDF\n",
    "X_text_KANSOAN_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3, X_test_KANSOAN, Y_train3, Y_test_KANSOAN = train_test_split(X_text_KANSOAN_, sentiment, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train31, X_test_KANSOAN1, Y_train31, Y_test_KANSOAN1 = train_test_split(df['text'], np.asarray(df['sentiment']), test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12670, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('./rotulação/TweetsPolitical01 OK OK.xlsx')\n",
    "df2 = pd.read_excel('./rotulação/TweetsPolitical02 OK OK.xlsx')\n",
    "df3 = pd.read_excel('./rotulação/TweetsPolitical03 OK OK.xlsx')\n",
    "df4 = pd.read_excel('./rotulação/TweetsPolitical04 OK OK.xlsx')\n",
    "df5 = pd.read_excel('./rotulação/TweetsPolitical05 OK OK.xlsx')\n",
    "df6 = pd.read_excel('./rotulação/TweetsPolitical06 OK OK.xlsx')\n",
    "df7 = pd.read_excel('./rotulação/TweetsPolitical07 OK OK.xlsx')\n",
    "df8 = pd.read_excel('./rotulação/TweetsPolitical08 OK OK.xlsx')\n",
    "df9 = pd.read_excel('./rotulação/TweetsPolitical09 OK OK.xlsx')\n",
    "df10 = pd.read_excel('./rotulação/TweetsPolitical10 OK OK.xlsx')\n",
    "df11 = pd.read_excel('./rotulação/TweetsPolitical11 OK OK.xlsx')\n",
    "df12 = pd.read_excel('./rotulação/TweetsPolitical12 OK OK.xlsx')\n",
    "df13 = pd.read_excel('./rotulação/TweetsPolitical13 OK OK.xlsx')\n",
    "df14 = pd.read_excel('./rotulação/TweetsPolitical14 OK OK.xlsx')\n",
    "df15 = pd.read_excel('./rotulação/TweetsPolitical15 OK OK.xlsx')\n",
    "\n",
    "lista = [df,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15]\n",
    "\n",
    "df_all = pd.concat(lista)\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1., -1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.drop(df_all[df_all['Polaridade'] == 11].index, inplace=True)\n",
    "df_all.drop(df_all[df_all['Polaridade'] == -2].index, inplace=True)\n",
    "df_all.drop(df_all[df_all['Polaridade'] == 10].index, inplace=True)\n",
    "\n",
    "df_all['Polaridade'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Polaridade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- #CaoNossoDeCadaDia #Novo - Vanessa Mandotti ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>- Bola pra frente que amanhã é outro dia, outr...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>- Cara de mal? Acho que não... Apenas um corte...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" #PCdoB O PARTIDO QUE FAZ A DIFERENÇA: LEAL E...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"@camaradamae: #PCdoB o Partido da coragem !  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Polaridade\n",
       "0  - #CaoNossoDeCadaDia #Novo - Vanessa Mandotti ...         0.0\n",
       "1  - Bola pra frente que amanhã é outro dia, outr...         1.0\n",
       "2  - Cara de mal? Acho que não... Apenas um corte...         1.0\n",
       "3  \" #PCdoB O PARTIDO QUE FAZ A DIFERENÇA: LEAL E...         1.0\n",
       "4  \"@camaradamae: #PCdoB o Partido da coragem !  ...         1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Tweet'] = df_all['Tweet'].apply(remove_user)\n",
    "df_all['Tweet'] = df_all['Tweet'].apply(Tokenize)\n",
    "polarity = np.asarray(df_all['Polaridade'])\n",
    "Tweet = df_all['Tweet']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12626, 25852)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "\n",
    "Tweet = count_vect.fit_transform(Tweet)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_transform = tfidf_transformer.fit_transform(Tweet) # Aplicando o TF-IDF\n",
    "X_train_transform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train_transform, polarity, test_size=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text, X_test_text, Y_train_text, Y_test_text = train_test_split(df_all['Tweet'] , df_all['Polaridade'], test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB - TRAIN -> TASH- TEST -> UNILEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.47      0.26      0.34      1202\n",
      "         0.0       0.42      0.66      0.51      1438\n",
      "         1.0       0.42      0.31      0.35      1148\n",
      "\n",
      "    accuracy                           0.43      3788\n",
      "   macro avg       0.44      0.41      0.40      3788\n",
      "weighted avg       0.43      0.43      0.41      3788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train2, Y_train2)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB - TRAIN -> TASH - TEST -> TWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.48      0.32      0.39       271\n",
      "           0       0.43      0.49      0.46       323\n",
      "           1       0.37      0.45      0.41       243\n",
      "\n",
      "    accuracy                           0.42       837\n",
      "   macro avg       0.43      0.42      0.42       837\n",
      "weighted avg       0.43      0.42      0.42       837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train2, Y_train2)\n",
    "y_pred = clf.predict(X_test_TASH)\n",
    "\n",
    "print(classification_report(Y_test_TASH,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB - TRAIN -> TASH - TEST -> KANSOAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(num):\n",
    "    if num == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "sentiment = df['sentiment'].apply(func)\n",
    "X_train3, X_test_KANSOAN, Y_train3, Y_test_KANSOAN = train_test_split(X_text_KANSOAN_, sentiment, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.92      0.53      0.67      1324\n",
      "           0       0.00      1.00      0.00         0\n",
      "           1       0.83      0.80      0.82      1444\n",
      "\n",
      "    accuracy                           0.67      2768\n",
      "   macro avg       0.58      0.78      0.50      2768\n",
      "weighted avg       0.87      0.67      0.75      2768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train2, Y_train2)\n",
    "y_pred = clf.predict(X_test_KANSOAN)\n",
    "\n",
    "print(classification_report(Y_test_KANSOAN,y_pred, zero_division=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - TRAIN->TASH - TEST->UNILEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.51      0.06      0.11      1202\n",
      "         0.0       0.39      0.94      0.56      1438\n",
      "         1.0       0.55      0.10      0.17      1148\n",
      "\n",
      "    accuracy                           0.41      3788\n",
      "   macro avg       0.48      0.37      0.28      3788\n",
      "weighted avg       0.48      0.41      0.30      3788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SVC().fit(X_train2, Y_train2) \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM T TASH T TWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.48      0.20      0.28      1017\n",
      "           0       0.29      0.74      0.41       866\n",
      "           1       0.64      0.33      0.44      1577\n",
      "\n",
      "    accuracy                           0.40      3460\n",
      "   macro avg       0.47      0.42      0.38      3460\n",
      "weighted avg       0.50      0.40      0.39      3460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SVC().fit(X_train2, Y_train2) \n",
    "y_pred = clf.predict(X_test_TWEET)\n",
    "\n",
    "print(classification_report(Y_test_TWEET,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM T TASH T KANSOAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      0.39      0.55      1324\n",
      "           0       0.00      1.00      0.00         0\n",
      "           1       0.88      0.71      0.79      1444\n",
      "\n",
      "    accuracy                           0.56      2768\n",
      "   macro avg       0.60      0.70      0.45      2768\n",
      "weighted avg       0.90      0.56      0.67      2768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SVC().fit(X_train2, Y_train2) \n",
    "y_pred = clf.predict(X_test_KANSOAN)\n",
    "\n",
    "print(classification_report(Y_test_KANSOAN,y_pred, zero_division=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL T TASH T UNILEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.49      0.17      0.25      1202\n",
      "         0.0       0.42      0.83      0.56      1438\n",
      "         1.0       0.49      0.22      0.31      1148\n",
      "\n",
      "    accuracy                           0.44      3788\n",
      "   macro avg       0.47      0.41      0.37      3788\n",
      "weighted avg       0.46      0.44      0.38      3788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000).fit(X_train2, Y_train2) \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL T TASH T TWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.47      0.27      0.34      1017\n",
      "           0       0.29      0.59      0.39       866\n",
      "           1       0.61      0.44      0.51      1577\n",
      "\n",
      "    accuracy                           0.43      3460\n",
      "   macro avg       0.46      0.43      0.41      3460\n",
      "weighted avg       0.49      0.43      0.43      3460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000).fit(X_train2, Y_train2) \n",
    "y_pred = clf.predict(X_test_TWEET)\n",
    "\n",
    "print(classification_report(Y_test_TWEET,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL T TASH T KANSOAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.92      0.55      0.69      1324\n",
      "           0       0.00      1.00      0.00         0\n",
      "           1       0.86      0.78      0.82      1444\n",
      "\n",
      "    accuracy                           0.67      2768\n",
      "   macro avg       0.59      0.78      0.50      2768\n",
      "weighted avg       0.89      0.67      0.76      2768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000).fit(X_train2, Y_train2) \n",
    "y_pred = clf.predict(X_test_KANSOAN)\n",
    "\n",
    "print(classification_report(Y_test_KANSOAN,y_pred, zero_division=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOTALMENT.C T TASH T UNILEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "61/61 [==============================] - 1s 21ms/step - loss: 1.0983 - accuracy: 0.3415 - val_loss: 1.0956 - val_accuracy: 0.3796\n",
      "Epoch 2/10\n",
      "61/61 [==============================] - 1s 14ms/step - loss: 1.0772 - accuracy: 0.4318 - val_loss: 1.0912 - val_accuracy: 0.3939\n",
      "Epoch 3/10\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 0.9843 - accuracy: 0.5569 - val_loss: 1.0734 - val_accuracy: 0.4205\n",
      "Epoch 4/10\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.7323 - accuracy: 0.7554 - val_loss: 1.1045 - val_accuracy: 0.4219\n",
      "Epoch 5/10\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.4730 - accuracy: 0.8600 - val_loss: 1.2364 - val_accuracy: 0.4271\n",
      "Epoch 6/10\n",
      "61/61 [==============================] - 1s 14ms/step - loss: 0.2862 - accuracy: 0.9272 - val_loss: 1.3989 - val_accuracy: 0.4219\n",
      "Epoch 7/10\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.1941 - accuracy: 0.9523 - val_loss: 1.5529 - val_accuracy: 0.4187\n",
      "Epoch 8/10\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.1397 - accuracy: 0.9677 - val_loss: 1.6818 - val_accuracy: 0.4161\n",
      "Epoch 9/10\n",
      "61/61 [==============================] - 1s 13ms/step - loss: 0.1035 - accuracy: 0.9841 - val_loss: 1.8569 - val_accuracy: 0.4195\n",
      "Epoch 10/10\n",
      "61/61 [==============================] - 1s 15ms/step - loss: 0.0854 - accuracy: 0.9779 - val_loss: 1.9274 - val_accuracy: 0.4097\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(25, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(10, activation='tanh'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(3 , activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "one = OneHotEncoder(sparse=False)\n",
    "\n",
    "y_one = one.fit_transform(np.asarray(Y_train2).reshape(-1,1))\n",
    "y_one_test = one.fit_transform(Y_test.reshape(-1,1))\n",
    "\n",
    "fit = model.fit(X_train2.todense(), y_one, epochs=10, validation_data=(X_test.todense(), y_one_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.44      0.42      1202\n",
      "           1       0.42      0.48      0.44      1438\n",
      "           2       0.41      0.30      0.34      1148\n",
      "\n",
      "    accuracy                           0.41      3788\n",
      "   macro avg       0.41      0.40      0.40      3788\n",
      "weighted avg       0.41      0.41      0.41      3788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(X_test.todense())\n",
    "\n",
    "print(classification_report(np.argmax(y_one_test, axis=1), np.argmax(predicted, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.32      0.35      1017\n",
      "           1       0.28      0.42      0.34       866\n",
      "           2       0.56      0.47      0.51      1577\n",
      "\n",
      "    accuracy                           0.41      3460\n",
      "   macro avg       0.41      0.40      0.40      3460\n",
      "weighted avg       0.44      0.41      0.42      3460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(X_test_TWEET.todense())\n",
    "\n",
    "print(classification_report(np.argmax(one.fit_transform(Y_test_TWEET.reshape(-1,1)), axis=1), np.argmax(predicted, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KANSOAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.65      0.72      1324\n",
      "           1       0.56      0.26      0.36      1444\n",
      "           2       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.45      2768\n",
      "   macro avg       0.46      0.30      0.36      2768\n",
      "weighted avg       0.68      0.45      0.53      2768\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ddayv\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(X_test_KANSOAN.todense())\n",
    "\n",
    "print(classification_report(np.argmax(one.fit_transform(np.asarray(Y_test_KANSOAN).reshape(-1,1)), axis=1), np.argmax(predicted, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM T TASH T TWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    " max_tokens=15000,\n",
    " output_mode='int',\n",
    " output_sequence_length=len(max(df1['text'])))\n",
    "\n",
    "vocab = set_array(df1['text'])\n",
    "\n",
    "vectorize_layer.adapt(np.unique(vocab))\n",
    "len(vectorize_layer.get_vocabulary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_KANSOAN\n",
    "\n",
    "X_train3_, X_test_KANSOAN_, Y_train3_, Y_test_KANSOAN_ = train_test_split(text_KANSOAN, df['sentiment'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 4s 242ms/step - loss: 1.0966 - accuracy: 0.3697 - val_loss: 1.1080 - val_accuracy: 0.2535\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 1.0940 - accuracy: 0.3723 - val_loss: 1.1104 - val_accuracy: 0.2535\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 1.0903 - accuracy: 0.3723 - val_loss: 1.1100 - val_accuracy: 0.2535\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 1.0748 - accuracy: 0.3985 - val_loss: 1.1084 - val_accuracy: 0.2754\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 1.0129 - accuracy: 0.5005 - val_loss: 1.1315 - val_accuracy: 0.2827\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.9056 - accuracy: 0.5636 - val_loss: 1.1413 - val_accuracy: 0.3153\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.8193 - accuracy: 0.6241 - val_loss: 1.1648 - val_accuracy: 0.3084\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.7261 - accuracy: 0.6928 - val_loss: 1.2005 - val_accuracy: 0.3237\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.6297 - accuracy: 0.7477 - val_loss: 1.3115 - val_accuracy: 0.3139\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.5600 - accuracy: 0.7810 - val_loss: 1.3455 - val_accuracy: 0.3335\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(vectorize_layer.get_vocabulary()),\n",
    "        output_dim=64,mask_zero=True),\n",
    "    \n",
    "    tf.keras.layers.LSTM(50, activation='relu' ,return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    tf.keras.layers.LSTM(25 , activation='tanh', return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    tf.keras.layers.LSTM(10 , activation='tanh'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer= tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "y_one = one.fit_transform(np.asarray(Y_train21).reshape(-1,1))\n",
    "y_one_test = one.fit_transform(np.asarray(Y_test_TWEET1).reshape(-1,1))\n",
    "\n",
    "fit = model.fit(np.asarray(pre_X(X_train21)), y_one, epochs=10, batch_size=128 ,validation_data=(np.asarray(pre_X(X_test_TWEET1)), y_one_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.21      0.27      1020\n",
      "           1       0.25      0.56      0.35       877\n",
      "           2       0.47      0.29      0.36      1563\n",
      "\n",
      "    accuracy                           0.33      3460\n",
      "   macro avg       0.37      0.35      0.32      3460\n",
      "weighted avg       0.39      0.33      0.33      3460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(np.asarray(np.asarray(pre_X(X_test_TWEET1))))\n",
    "\n",
    "print(classification_report(np.argmax(one.fit_transform(np.asarray(Y_test_TWEET1).reshape(-1,1)), axis=1), np.argmax(predicted, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNILEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.29      0.33      1239\n",
      "           1       0.42      0.59      0.49      1431\n",
      "           2       0.37      0.26      0.31      1118\n",
      "\n",
      "    accuracy                           0.40      3788\n",
      "   macro avg       0.39      0.38      0.38      3788\n",
      "weighted avg       0.39      0.40      0.38      3788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(np.asarray(pre_X(X_test_text)))\n",
    "\n",
    "print(classification_report(np.argmax(one.fit_transform(np.asarray(Y_test_text).reshape(-1,1)), axis=1), np.argmax(predicted, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KANSOAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.33      0.45      1335\n",
      "           1       0.58      0.53      0.55      1433\n",
      "           2       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.43      2768\n",
      "   macro avg       0.44      0.28      0.34      2768\n",
      "weighted avg       0.66      0.43      0.50      2768\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ddayv\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(np.asarray(pre_X(X_test_KANSOAN1)))\n",
    "\n",
    "print(classification_report(np.argmax(one.fit_transform(np.asarray(Y_test_KANSOAN1).reshape(-1,1)), axis=1), np.argmax(predicted, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONV1D T TASH T KANSOAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.0975 - accuracy: 0.3626\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.0909 - accuracy: 0.3723\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.0839 - accuracy: 0.3738\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.0710 - accuracy: 0.4313 0s - loss: 1.0729 - accuracy: 0.\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.0408 - accuracy: 0.4969\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.9763 - accuracy: 0.5856\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.8639 - accuracy: 0.6133\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.7428 - accuracy: 0.6410\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6343 - accuracy: 0.7374\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5407 - accuracy: 0.8185\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(vectorize_layer.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        mask_zero=True),\n",
    "    \n",
    "    tf.keras.layers.Conv1D(32,6, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')   \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer= tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "y_one = one.fit_transform(np.asarray(Y_train21).reshape(-1,1))\n",
    "y_one_test = one.fit_transform(Y_test_KANSOAN1.reshape(-1,1))\n",
    "\n",
    "fit = model.fit(np.asarray(pre_X(X_train21)), y_one, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.41      0.44      1335\n",
      "           1       0.50      0.43      0.46      1433\n",
      "           2       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.42      2768\n",
      "   macro avg       0.32      0.28      0.30      2768\n",
      "weighted avg       0.48      0.42      0.45      2768\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ddayv\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(np.asarray(pre_X(X_test_KANSOAN1)))\n",
    "\n",
    "print(classification_report(np.argmax(one.fit_transform(np.asarray(Y_test_KANSOAN1).reshape(-1,1)), axis=1), np.argmax(predicted, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNILEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.40      0.35      1239\n",
      "           1       0.35      0.46      0.40      1431\n",
      "           2       0.34      0.10      0.15      1118\n",
      "\n",
      "    accuracy                           0.33      3788\n",
      "   macro avg       0.34      0.32      0.30      3788\n",
      "weighted avg       0.34      0.33      0.31      3788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(np.asarray(pre_X(X_test_text)))\n",
    "\n",
    "print(classification_report(np.argmax(one.fit_transform(np.asarray(Y_test_text).reshape(-1,1)), axis=1), np.argmax(predicted, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.44      0.33      1020\n",
      "           1       0.27      0.44      0.33       877\n",
      "           2       0.49      0.12      0.19      1563\n",
      "\n",
      "    accuracy                           0.29      3460\n",
      "   macro avg       0.34      0.33      0.28      3460\n",
      "weighted avg       0.37      0.29      0.27      3460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(np.asarray(pre_X(X_test_TWEET1)))\n",
    "\n",
    "print(classification_report(np.argmax(one.fit_transform(np.asarray(Y_test_TWEET1).reshape(-1,1)), axis=1), np.argmax(predicted, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BDR T TASH T UNILEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 1.0983 - accuracy: 0.3369 - val_loss: 1.0967 - val_accuracy: 0.3757\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 1.0969 - accuracy: 0.3723 - val_loss: 1.0963 - val_accuracy: 0.3778\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 1.0955 - accuracy: 0.3723 - val_loss: 1.0958 - val_accuracy: 0.3778\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 1.0942 - accuracy: 0.3723 - val_loss: 1.0952 - val_accuracy: 0.3778\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 1.0928 - accuracy: 0.3723 - val_loss: 1.0948 - val_accuracy: 0.3778\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 1.0911 - accuracy: 0.3723 - val_loss: 1.0945 - val_accuracy: 0.3778\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 1.0889 - accuracy: 0.3723 - val_loss: 1.0943 - val_accuracy: 0.3778\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1.0861 - accuracy: 0.3728 - val_loss: 1.0945 - val_accuracy: 0.3780\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 1.0816 - accuracy: 0.3754 - val_loss: 1.0944 - val_accuracy: 0.3783\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 1.0756 - accuracy: 0.3903 - val_loss: 1.0946 - val_accuracy: 0.3796\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(vectorize_layer.get_vocabulary()),\n",
    "        output_dim=1,\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1)),\n",
    "\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer= tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "y_one = one.fit_transform(np.asarray(Y_train21).reshape(-1,1))\n",
    "y_one_test = one.fit_transform(np.asarray(Y_test_text).reshape(-1,1))\n",
    "\n",
    "fit = model.fit(np.asarray(pre_X(X_train21)), y_one, epochs=10, batch_size=128 ,validation_data=(np.asarray(pre_X(X_test_text)), y_one_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1239\n",
      "           1       0.38      0.99      0.55      1431\n",
      "           2       0.52      0.02      0.04      1118\n",
      "\n",
      "    accuracy                           0.38      3788\n",
      "   macro avg       0.30      0.34      0.20      3788\n",
      "weighted avg       0.30      0.38      0.22      3788\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ddayv\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(np.asarray(pre_X(X_test_text)))\n",
    "\n",
    "print(classification_report(np.argmax(one.fit_transform(np.asarray(Y_test_text).reshape(-1,1)), axis=1), np.argmax(predicted, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1020\n",
      "           1       0.25      0.99      0.40       877\n",
      "           2       0.44      0.01      0.02      1563\n",
      "\n",
      "    accuracy                           0.26      3460\n",
      "   macro avg       0.23      0.33      0.14      3460\n",
      "weighted avg       0.27      0.26      0.11      3460\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ddayv\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(np.asarray(pre_X(X_test_TWEET1)))\n",
    "\n",
    "print(classification_report(np.argmax(one.fit_transform(np.asarray(Y_test_TWEET1).reshape(-1,1)), axis=1), np.argmax(predicted, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KANSOAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1335\n",
      "           1       0.52      0.99      0.68      1433\n",
      "           2       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.51      2768\n",
      "   macro avg       0.17      0.33      0.23      2768\n",
      "weighted avg       0.27      0.51      0.35      2768\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ddayv\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\ddayv\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(np.asarray(pre_X(X_test_KANSOAN1)))\n",
    "\n",
    "print(classification_report(np.argmax(one.fit_transform(np.asarray(Y_test_KANSOAN1).reshape(-1,1)), axis=1), np.argmax(predicted, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
